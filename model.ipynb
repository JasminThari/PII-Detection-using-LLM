{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the python version to console\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (AutoTokenizer, \n",
    "                          AutoModelForTokenClassification, \n",
    "                          DataCollatorForTokenClassification, \n",
    "                          TrainingArguments, \n",
    "                          Trainer,\n",
    "                          pipeline)\n",
    "import evaluate\n",
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "#cuda test\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your JSON file\n",
    "path_to_json = \"data/train.json\"\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('json', data_files={'train': path_to_json})\n",
    "\n",
    "# use the following dictionary to map strings to integers\n",
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-NAME_STUDENT\": 1,\n",
    "    \"I-NAME_STUDENT\": 2,\n",
    "    \"B-ID_NUM\": 3,\n",
    "    \"I-ID_NUM\": 4,\n",
    "    \"B-PHONE_NUM\": 5,\n",
    "    \"I-PHONE_NUM\": 6,\n",
    "    \"B-EMAIL\": 7,\n",
    "    \"I-EMAIL\": 8,\n",
    "    \"B-URL_PERSONAL\": 9,\n",
    "    \"I-URL_PERSONAL\": 10,\n",
    "    \"B-STREET_ADDRESS\": 11,\n",
    "    \"I-STREET_ADDRESS\": 12,\n",
    "    \"B-USERNAME\": 13,\n",
    "    \"I-USERNAME\": 14,\n",
    "}\n",
    "\n",
    "# reverse the dictionary label2id\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "# use the dict to map the strings to integers\n",
    "dataset = dataset.map(lambda x: {'labels_int': [label2id[i] for i in x['labels']]})\n",
    "\n",
    "# Split the dataset into a training, validation and test dataset\n",
    "data_dict = dataset['train'].train_test_split(test_size=0.1)\n",
    " \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce size of both train, validation and test datasets\n",
    "data_size = 100\n",
    "\n",
    "if data_size is not None:\n",
    "    data_dict[\"train\"] = data_dict[\"train\"].select(range(data_size))\n",
    "    #data_dict[\"validation\"] = data_dict[\"validation\"].select(range(data_size))\n",
    "    data_dict[\"test\"] = data_dict[\"test\"].select(range(data_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/100 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 100/100 [00:01<00:00, 79.15 examples/s] \n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 139.16 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# find the labels\n",
    "#label_list = wnut[\"train\"].features[\"ner_tags\"].feature.names\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\") # might be better to used the cased once, since we are doin NER\n",
    "\n",
    "# function that tokenizes according the chosen model and aligns the labels with the new tokens\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"labels_int\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100. # -100 is the default value for ignore_index in CrossEntropyLoss.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"transformed_labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_data_dict = data_dict.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['trailing_whitespace', 'labels', 'tokens', 'document', 'full_text', 'labels_int', 'input_ids', 'attention_mask', 'transformed_labels'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data_dict[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "#labels = [label_list[i] for i in example[f\"ner_tags\"]]\n",
    "\n",
    "label_list = list(label2id.keys())\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\", num_labels=len(label2id.keys()), id2label=id2label, label2id=label2id)\n",
    "# move the model to the device\n",
    "model.to(device)\n",
    "\n",
    "# define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"ner_pii_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    save_steps=1000,\n",
    ")\n",
    "\n",
    "# define the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=data_dict[\"train\"].map(tokenize_and_align_labels, batched=True),\n",
    "    eval_dataset=data_dict[\"test\"].map(tokenize_and_align_labels, batched=True),\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# save the model\n",
    "model.save_pretrained(\"saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bachelor_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
