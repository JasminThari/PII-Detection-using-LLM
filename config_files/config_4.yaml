name: config_4
model_name: "distilbert/distilbert-base-cased"

training_args: 
  learning_rate: 0.00002
  per_device_train_batch_size: 32 # 32 works on 32gb gpu and so does 64, chech hugginface for model usage
  per_device_eval_batch_size: 32
  num_train_epochs: 50
  weight_decay: 0.01
  evaluation_strategy: "epoch"
  save_strategy: "epoch"
  save_total_limit: 4  # limit the total amount of checkpoints, it will delete the older checkpoints
  load_best_model_at_end: True
  push_to_hub: False

data_prep: 
  df_path: data/data_df.json
  test_size: 0.2 # proportion of the orginal data to be used for testing
  val_size: 0.2 # proportion of the training data to be used for validation
  random_state: 42
  use_LLM_data_train: True
  use_LLM_data_test: False
  use_cased_model: True
  max_length: 512
  chunk_data: False
  #tokenizer_name: "${self:model}" #this is not used
  label2id:  {
    "O": 0,
    "B-NAME_STUDENT": 1,
    "I-NAME_STUDENT": 2,
    "B-ID_NUM": 3,
    "I-ID_NUM": 4,
    "B-PHONE_NUM": 5,
    "I-PHONE_NUM": 6,
    "B-EMAIL": 7,
    "I-EMAIL": 8,
    "B-URL_PERSONAL": 9,
    "I-URL_PERSONAL": 10,
    "B-STREET_ADDRESS": 11,
    "I-STREET_ADDRESS": 12,
    "B-USERNAME": 13,
    "I-USERNAME": 14}

