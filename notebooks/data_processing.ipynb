{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import json \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import re\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from itertools import chain\n",
    "import ast\n",
    "from IPython.display import HTML\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.tokens import Doc, Span\n",
    "spacy_nlp = spacy.load('en_core_web_sm')\n",
    "from spacy.lang.en import English\n",
    "import string\n",
    "eng_tokenizer = English().tokenizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data in dictionary and dataframe format\n",
    "with open('../data/train.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "data_df = pd.read_json('../data/train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of documents:',len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess data\n",
    "data_df = data_df.rename(columns={'full_text':'text'})\n",
    "data_df['llm_generated'] = False\n",
    "data_df['prompt_id'] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load llm generated data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load more data (llm generated)\n",
    "pii_dataset = pd.read_csv('../data/pii_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pii_dataset.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of documents:',len(pii_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data\n",
    "pii_dataset['llm_generated'] = True\n",
    "\n",
    "# Convert string to list\n",
    "pii_dataset[[\"tokens\", \"trailing_whitespace\", \"labels\"]] = pii_dataset[[\"tokens\", \"trailing_whitespace\", \"labels\"]].map(ast.literal_eval)\n",
    "pii_dataset[\"document\"] = pii_dataset[\"document\"].astype(\"category\").cat.codes + (data_df.document.max() + 1) # make sure document id is unique and changing to int\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize using spacy (to have same tokenizer as the original data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_spacy(text, tokenizer=eng_tokenizer):\n",
    "    tokenized_text = tokenizer(text)\n",
    "    tokens = [token.text for token in tokenized_text]\n",
    "    trailing_whitespace = [bool(token.whitespace_) for token in tokenized_text]\n",
    "    return {'tokens': tokens, 'trailing_whitespace': trailing_whitespace}\n",
    "\n",
    "def create_new_tokens_labels(row):\n",
    "    \n",
    "    tokens, labels, trailing_whitespace = row.tokens, row.labels, row.trailing_whitespace\n",
    "    new_tokens, new_labels, new_trailing_whitespaces = [], [], []\n",
    "    labels = [l.split(\"-\")[1] if l != \"O\" else l for l in labels]\n",
    "    \n",
    "    for i in range(len(tokens)):\n",
    "        t = tokens[i]\n",
    "        l = labels[i]\n",
    "        ws = trailing_whitespace[i]\n",
    "        \n",
    "        prev_l = labels[i - 1] if i > 0 else \"O\"\n",
    "        next_l = labels[i + 1] if (i + 1) < len(labels) else \"O\"\n",
    "        \n",
    "        # Found a PHONE_NUM token mislabed as STREET_ADDRESS:\n",
    "        if l != \"O\" and re.search(r'\\+\\d+', t):\n",
    "            l = \"PHONE_NUM\"\n",
    "        \n",
    "        # Found STREET_ADDRESS between 2 PHONE_NUM:\n",
    "        if l == \"STREET_ADDRESS\" and prev_l == \"PHONE_NUM\" and prev_l == next_l:\n",
    "            l = \"PHONE_NUM\"\n",
    "            \n",
    "        # Found individual mislabeled STREET_ADDRESS tokens:\n",
    "        elif l == \"STREET_ADDRESS\" and l != next_l and l != prev_l:\n",
    "            l = \"O\"\n",
    "        \n",
    "        # Create spacy tokens and their labels\n",
    "        tok_ = tokenize_with_spacy(t)\n",
    "        spacy_tokens = tok_[\"tokens\"]\n",
    "        new_tokens.extend(spacy_tokens)\n",
    "        \n",
    "        new_labels.extend([l if st not in string.punctuation else \"O\" for st in spacy_tokens])\n",
    "        \n",
    "        new_trailing_whitespaces.extend(tok_[\"trailing_whitespace\"])\n",
    "        new_trailing_whitespaces[-1] = ws  \n",
    "        \n",
    "    return pd.Series({\"document\": row.document, \"tokens\": new_tokens, \"trailing_whitespace\": new_trailing_whitespaces, \"labels\": new_labels})\n",
    "\n",
    "\n",
    "def update_labels(row):    \n",
    "    tokens = row.tokens\n",
    "    labels = row.labels\n",
    "    new_labels = [\"O\"] * len(labels)\n",
    "    for i, (t, l) in enumerate(zip(tokens, labels)):\n",
    "        prev_l = new_labels[i - 1] if i > 0 else \"O\"\n",
    "        next_l = labels[i + 1] if i + 1 < len(labels) else \"O\"\n",
    "    \n",
    "        if (prev_l == \"NAME_STUDENT\" or prev_l == \"O\") and t == \"'s\":\n",
    "            new_labels[i] = \"O\"\n",
    "\n",
    "        elif t == \"(\" and next_l == \"PHONE_NUM\":\n",
    "            new_labels[i] = next_l\n",
    "\n",
    "        elif t == \")\" and prev_l == \"PHONE_NUM\":\n",
    "            new_labels[i] = prev_l\n",
    "        \n",
    "        elif (t in string.punctuation) and (prev_l == next_l) and (prev_l != \"O\"):\n",
    "            \n",
    "            if t == \",\" and prev_l != \"STREET_ADDRESS\":\n",
    "                new_labels[i] = \"O\"\n",
    "            elif t == \".\" and prev_l == \"NAME_STUDENT\":\n",
    "                new_labels[i] = \"O\"\n",
    "            else:\n",
    "                new_labels[i] = prev_l\n",
    "        \n",
    "        else:\n",
    "            new_labels[i] = l\n",
    "    \n",
    "    return new_labels\n",
    "\n",
    "def create_bio_labels(labels):\n",
    "    new_labels = [\"O\"]*len(labels)\n",
    "    prev_l = \"O\"\n",
    "    for i, l in enumerate(labels):\n",
    "        if l != \"O\":\n",
    "            if l != prev_l:\n",
    "                new_labels[i] = \"B-\" + l\n",
    "            elif l == prev_l:\n",
    "                new_labels[i] = \"I-\" + l\n",
    "        prev_l = l\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pii_dataset = pii_dataset.apply(create_new_tokens_labels, axis=1)\n",
    "new_pii_dataset[\"labels\"] = new_pii_dataset.apply(refine_punctuation_labels, axis=1).apply(create_bio_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pii_dataset['text'] = new_pii_dataset['tokens'].apply(lambda x: ' '.join(x))\n",
    "new_pii_dataset[['prompt_id', 'llm_generated']] = pii_dataset[['prompt_id', 'llm_generated']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pii_dataset.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the two datasets\n",
    "df = pd.concat([data_df, new_pii_dataset], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(df):\n",
    "    df = df.copy()\n",
    "    df[\"unique_labels\"] = df[\"labels\"].apply(lambda x: set(\n",
    "        [l.split('-')[1] if l != 'O' else l for l in x]\n",
    "         ))\n",
    "\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    one_hot_encoded = mlb.fit_transform(df['unique_labels'])\n",
    "    one_hot_df = pd.DataFrame(one_hot_encoded, columns=mlb.classes_)\n",
    "    df = pd.concat([df, one_hot_df], axis=1)\n",
    "    \n",
    "    # add 'OTHER' column which is only true when we have no other label in text\n",
    "    df['OTHER'] = df['unique_labels'].apply(lambda x: 1 if len(x - {\"O\"}) == 0 else 0)\n",
    "    \n",
    "    return df, list(mlb.classes_) + ['OTHER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, label_classes = encode_labels(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target distribution plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df['labels'].tolist()\n",
    "\n",
    "# Flatten the list of lists\n",
    "flattened_labels = list(itertools.chain.from_iterable(labels))\n",
    "\n",
    "# Count the occurrences of each label\n",
    "label_counts = Counter(flattened_labels)\n",
    "\n",
    "# Separate the labels and their counts for plotting\n",
    "labels, counts = zip(*label_counts.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the bar plot\n",
    "fig = go.Figure([go.Bar(x=labels[1:], y=counts[1:])])\n",
    "\n",
    "# Customize the layout\n",
    "fig.update_layout(title_text='Frequency of Each Label', xaxis_title='Labels', yaxis_title='Frequency')\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unique targets in each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['unique_labels'] = df['labels'].apply(lambda x: list(set(x)))\n",
    "df['num_labels'] = df['unique_labels'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram of number of labels per document\n",
    "fig = px.histogram(df, x='num_labels', nbins=20, title='Histogram of number of unique labels per document')\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of tokens in each document (with whitespace) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_tokens'] = df['tokens'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram of number of tokens per document\n",
    "fig = px.histogram(df, x='num_tokens', nbins=500, title='Histogram of number of tokens per document')\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of text length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['len_text'] = df['text'].apply(len)\n",
    "fig = px.histogram(df, x='len_text', nbins=500, title='Histogram of length of text per document')\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of documents without labels(only \"o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_outer = df[df['labels'].apply(lambda x: len(set(x)) > 1)] #with labels\n",
    "df_outer = df[df['labels'].apply(lambda x: 'O' in x and len(set(x)) == 1)] #without labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_outer['Label Type'] = 'With Labels'\n",
    "df_outer['Label Type'] = 'Without Labels'\n",
    "\n",
    "# Calculate text length\n",
    "df['len_text'] = df['text'].apply(len)\n",
    "df_non_outer['len_text'] = df_non_outer['text'].apply(len)\n",
    "df_outer['len_text'] = df_outer['text'].apply(len)\n",
    "\n",
    "# Combine the dataframes\n",
    "combined_df = pd.concat([df_non_outer, df_outer])\n",
    "\n",
    "# Plotting\n",
    "fig = px.histogram(combined_df, x='len_text', color='Label Type', barmode='overlay',\n",
    "                   nbins=500, title='Histogram of Length of Text per Document')\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the dataframes\n",
    "combined_df = pd.concat([df_non_outer, df_outer])\n",
    "combined_df['len_tokens'] = combined_df['tokens'].apply(len)\n",
    "# Plotting\n",
    "fig = px.histogram(combined_df, x='len_tokens', color='Label Type', barmode='overlay',\n",
    "                   nbins=500, title='Histogram of Length of Tokens per Document')\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels Pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"labels_pos\"] = df[\"labels\"].apply(lambda labels: np.arange(1, len(labels) + 1) / len(labels))\n",
    "exp_df = df.explode([\"tokens\", \"labels\", \"labels_pos\"])\n",
    "exp_df[\"labels\"] = pd.Categorical(exp_df[\"labels\"], categories=labels, ordered=True)\n",
    "exp_df = exp_df.sort_values(by=\"labels\", ascending=False)\n",
    "label_tokens = exp_df.groupby(\"labels\", observed=False).agg(list)\n",
    "label_tokens[\"counts\"] = label_tokens[\"tokens\"].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(exp_df, x='labels_pos', y='labels', title='Scatter Plot of Labels in Documents',)\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title='X Axis Label',\n",
    "    yaxis_title='Y Axis Label',\n",
    "    legend_title='Legend'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word cloud for each target \n",
    "# wordcloud for surronding words of each target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from wordcloud import WordCloud\n",
    "# wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(df_train['full_text']))\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.imshow(wordcloud, interpolation='bilinear')\n",
    "# plt.axis('off')\n",
    "# plt.title('Word Cloud of Essays')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER Visualization with Spacy of train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_spacy_format(text, tokens, labels, trailing_whitespace):\n",
    "    ents = []  # To store entity dictionaries\n",
    "    start = 0  # Position tracker for the start of each token in the text\n",
    "    \n",
    "    for i, (token, label, space) in enumerate(zip(tokens, labels, trailing_whitespace)):\n",
    "        if label.startswith('B-') or label.startswith('I-'):\n",
    "            label_type = label[2:]  # Extract entity type from label\n",
    "            token_start = text.find(token, start)  # Find the start index of the token in text\n",
    "            token_end = token_start + len(token)  # Calculate the end index of the token\n",
    "            \n",
    "            # If it's a 'B-' label or the first 'I-' label following non-matching or 'O' labels, start a new entity\n",
    "            if label.startswith('B-') or (label.startswith('I-') and (i == 0 or not labels[i-1].endswith(label_type))):\n",
    "                ents.append({\"start\": token_start, \"end\": token_end, \"label\": label_type})\n",
    "            # If it's an 'I-' label continuing an entity, extend the last entity's end index\n",
    "            elif label.startswith('I-') and ents and ents[-1][\"label\"] == label_type:\n",
    "                ents[-1][\"end\"] = token_end\n",
    "            \n",
    "            start = token_end + (1 if space == 'True' else 0)  # Update start position for next token\n",
    "    \n",
    "    return [{\"text\": text, \"ents\": ents, \"title\": None}]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_ner(row):\n",
    "\n",
    "    display_text = row['text'].values[0] \n",
    "    display_labels = row['labels'].values[0] \n",
    "    trailing_whitespace = row['trailing_whitespace'].values[0] \n",
    "    tokens = row['tokens'].values[0] \n",
    "\n",
    "    display_text = display_text.replace(\"\\n\\n\", \"\\r\\n\")\n",
    "    ex = convert_to_spacy_format(display_text, tokens, display_labels, trailing_whitespace)\n",
    "\n",
    "    custom_css = \"\"\"\n",
    "                <style>    \n",
    "                    /* Customizing entity appearance */\n",
    "                    .entities {\n",
    "                        font-size: 11px !important;\n",
    "                        font-family: Verdana !important;\n",
    "                        line-height: 1.25 !important;\n",
    "                        border-radius: 10px !important; /* Rounded corners */\n",
    "                        background-color: #f9f9f9 !important; /* Very light gray background */\n",
    "                        padding: 20px 15px !important; /* Adjust padding */\n",
    "                    }\n",
    "                    /* Customizing entity appearance */\n",
    "                    .entity {\n",
    "                        font-size: 10px !important;\n",
    "                        padding: 0.2em 0.4em !important;\n",
    "                        font-family: Verdana !important;\n",
    "                        font-weight: bold !important;\n",
    "                        \n",
    "                    }\n",
    "                </style>\n",
    "                \"\"\"\n",
    "\n",
    "    options = {\"colors\": {\"NAME_STUDENT\": \"#748CAB\", \"URL_PERSONAL\": \"#FFFC31\", \n",
    "                        \"ID_NUM\": \"#E94F37\", \"EMAIL\": \"#F8B195\", \"STREET_ADDRESS\": \"#BDBF09\", \"PHONE_NUM\": \"#D96C06\", \"USERNAME\": \"#2292A4\"}}\n",
    "\n",
    "    # Inject custom CSS\n",
    "    display(HTML(custom_css))\n",
    "\n",
    "    spacy.displacy.render(ex, style=\"ent\", manual=True, jupyter=True, options=options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_ner(df[df['document'] == 9854])\n",
    "visualize_ner(df.sort_values(by=[\"unique_labels\"], ascending=True).reset_index(drop=True).iloc[0:1])\n",
    "visualize_ner(df.sort_values(by=[\"num_labels\"], ascending=False).reset_index(drop=True).iloc[0:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word surronding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pii_data = {\n",
    "#     'EMAIL':{},\n",
    "#     'ID_NUM':{},\n",
    "#     'NAME_STUDENT':{},\n",
    "#     'PHONE_NUM':{},\n",
    "#     'STREET_ADDRESS':{},\n",
    "#     'URL_PERSONAL':{},\n",
    "#     'USERNAME':{},\n",
    "# }\n",
    "\n",
    "# def update_pii_data(pii_type, token, tokens, i):\n",
    "#     if pii_type not in pii_data:\n",
    "#         pii_data[pii_type] = {}\n",
    "    \n",
    "#     # Extracting the token and surrounding context\n",
    "#     token_text = tokens[i]\n",
    "#     surrounding_tokens = tokens[max(0, i-2):i] + tokens[i+1:min(len(tokens), i+3)]\n",
    "#     sentence = ' '.join(tokens)  # Simplified; consider a more accurate sentence detection\n",
    "    \n",
    "#     # Assuming `tokens` is a list of all tokens in the document and `i` is the index of the current token\n",
    "#     sentence_boundaries = [j for j, token in enumerate(tokens) if token in '.!?'] + [len(tokens)-1]\n",
    "#     sentence_start = max([boundary for boundary in sentence_boundaries if boundary < i]+[0])\n",
    "#     sentence_end = min([boundary for boundary in sentence_boundaries if boundary > i]+[len(tokens)-1])\n",
    "#     sentence_context = tokens[sentence_start:sentence_end+1]\n",
    "    \n",
    "#     # Determine PII position in the sentence\n",
    "#     position_in_sentence = \"middle\"\n",
    "#     if i == sentence_start:\n",
    "#         position_in_sentence = \"beginning\"\n",
    "#     elif i == sentence_end:\n",
    "#         position_in_sentence = \"end\"\n",
    "    \n",
    "#     # PII Token Type\n",
    "#     if token_text.isalpha():\n",
    "#         token_type = \"alphabetic\"\n",
    "#     elif token_text.isdigit():\n",
    "#         token_type = \"numeric\"\n",
    "#     else:\n",
    "#         token_type = \"alphanumeric\" if any(char.isalpha() for char in token_text) else \"other\"\n",
    "    \n",
    "#     # PII Format Pattern\n",
    "#     format_pattern = ''.join(['d' if char.isdigit() else 'l' if char.isalpha() else char for char in token_text])\n",
    "    \n",
    "#     # Capitalization\n",
    "#     capitalization = \"lowercase\"\n",
    "#     if token_text.isupper():\n",
    "#         capitalization = \"uppercase\"\n",
    "#     elif token_text.istitle():\n",
    "#         capitalization = \"titlecase\"\n",
    "    \n",
    "#     # Special Characters\n",
    "#     special_chars = any(not char.isalnum() for char in token_text)\n",
    "    \n",
    "#     details = {\n",
    "#         'token_text': token_text,\n",
    "#         'surrounding_words': surrounding_tokens,\n",
    "#         'location_in_essay': i,\n",
    "#         'sentence_context': ' '.join(sentence_context),\n",
    "#         'pii_length': len(token_text),\n",
    "#         'position_in_sentence': position_in_sentence,\n",
    "#         'token_type': token_type,\n",
    "#         'format_pattern': format_pattern,\n",
    "#         'capitalization': capitalization,\n",
    "#         'special_chars': special_chars,\n",
    "#         # Add more details as needed\n",
    "#     }\n",
    "    \n",
    "#     pii_token_key = f\"{token_text}_{i}\"  # Unique key for each PII instance\n",
    "#     if pii_token_key not in pii_data[pii_type]:\n",
    "#         pii_data[pii_type][pii_token_key] = []\n",
    "#     pii_data[pii_type][pii_token_key].append(details)\n",
    "\n",
    "# # Example of how to call this function within your iteration over the DataFrame\n",
    "# for index, row in tqdm(train_df.iterrows(), total=len(train_df)):\n",
    "#     tokens = row['tokens']\n",
    "#     labels = row['labels']\n",
    "#     for i, label in enumerate(labels):\n",
    "#         if label != 'O':  # If the label indicates PII\n",
    "#             pii_type = label[2:]  # Extract PII type (removing the B- or I- prefix)\n",
    "#             update_pii_data(pii_type, tokens[i], tokens, i)\n",
    "            \n",
    "            \n",
    "# # Initialize an empty list to hold each PII instance as a dictionary\n",
    "# flat_pii_data = []\n",
    "\n",
    "# # Iterate through the pii_data dictionary\n",
    "# for pii_type, pii_instances in pii_data.items():\n",
    "#     for instance_key, details_list in pii_instances.items():\n",
    "#         for details in details_list:\n",
    "#             # Create a flat dictionary for each PII instance\n",
    "#             flat_instance = details.copy()  # Start with the existing details\n",
    "#             flat_instance['pii_type'] = pii_type  # Add the PII type\n",
    "#             flat_instance['instance_key'] = instance_key  # Add the instance key for reference\n",
    "            \n",
    "#             # Append this flat dictionary to our list\n",
    "#             flat_pii_data.append(flat_instance)\n",
    "\n",
    "# # Store and viz\n",
    "# train_pii_df = pd.DataFrame(flat_pii_data)\n",
    "# print(\"\\n... PII DATAFRAME ...\\n\")\n",
    "# display(train_pii_df)\n",
    "\n",
    "# # Count the number of instances for each PII type\n",
    "# pii_type_counts = train_pii_df['pii_type'].value_counts()\n",
    "# print(\"\\n... PII TYPE COUNTS ...\\n\")\n",
    "# display(pii_type_counts.to_frame().T)\n",
    "\n",
    "# # Or aggregate to find the average PII length by type\n",
    "# average_pii_length_by_type = train_pii_df.groupby('pii_type')['pii_length'].mean()\n",
    "# print(\"\\n\\n... AVERAGE PII LENGTH (CHARS) BY TYPE ...\\n\")\n",
    "# display(average_pii_length_by_type.to_frame().T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Detect_PII",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
