{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import json \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import re\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from itertools import chain\n",
    "import ast\n",
    "from IPython.display import HTML\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.tokens import Doc, Span\n",
    "spacy_nlp = spacy.load('en_core_web_sm')\n",
    "from spacy.lang.en import English\n",
    "import string\n",
    "en_tokenizer = English().tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data in dictionary and dataframe format\n",
    "with open('../data/train.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "data_df = pd.read_json('../data/train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of documents:',len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess data\n",
    "data_df = data_df.rename(columns={'full_text':'text'})\n",
    "data_df['llm_generated'] = False\n",
    "data_df['prompt_id'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load llm generated data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load more data (llm generated)\n",
    "pii_dataset = pd.read_csv('../data/pii_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pii_dataset.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of documents:',len(pii_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data\n",
    "pii_dataset['llm'] = True\n",
    "\n",
    "# Convert string to list\n",
    "pii_dataset[[\"tokens\", \"trailing_whitespace\", \"labels\"]] = pii_dataset[[\"tokens\", \"trailing_whitespace\", \"labels\"]].map(ast.literal_eval)\n",
    "pii_dataset[\"document\"] = pii_dataset[\"document\"].astype(\"category\").cat.codes + (data_df.document.max() + 1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_spacy(text, tokenizer=en_tokenizer):\n",
    "    tokenized_text = tokenizer(text)\n",
    "    tokens = [token.text for token in tokenized_text]\n",
    "    trailing_whitespace = [bool(token.whitespace_) for token in tokenized_text]\n",
    "    return {'tokens': tokens, 'trailing_whitespace': trailing_whitespace}\n",
    "\n",
    "def create_new_tokens_labels(row):\n",
    "    '''\n",
    "    We will convert tokens to spacy tokens. \n",
    "    This way we can have multiple spacy tokens with different labels seperated by no trailing whitespace. \n",
    "    If original tokens have always spaces, then tokenizing them individually to spacy tokens and then combining will be exactly \n",
    "    same sa tokenizing the whole text created by the original tokens.\n",
    "    The labels are created such that if spacy token is punctuation, then it has 'O' label for time being.\n",
    "    Also handle common mislabeled STREET-ADDRESS.\n",
    "    '''\n",
    "    \n",
    "    tokens, labels, trailing_whitespace = row.tokens, row.labels, row.trailing_whitespace\n",
    "    new_tokens, new_labels, new_trailing_whitespaces = [], [], []\n",
    "    labels = [l.split(\"-\")[1] if l != \"O\" else l for l in labels]\n",
    "    \n",
    "    for i in range(len(tokens)):\n",
    "        t = tokens[i]\n",
    "        l = labels[i]\n",
    "        ws = trailing_whitespace[i]\n",
    "        \n",
    "        prev_l = labels[i - 1] if i > 0 else \"O\"\n",
    "        next_l = labels[i + 1] if (i + 1) < len(labels) else \"O\"\n",
    "        \n",
    "        # Found a PHONE_NUM token mislabed as STREET_ADDRESS so make sure:\n",
    "        if l != \"O\" and re.search(r'\\+\\d+', t):\n",
    "            l = \"PHONE_NUM\"\n",
    "        \n",
    "        # Found STREET_ADDRESS between 2 PHONE_NUM so make sure:\n",
    "        if l == \"STREET_ADDRESS\" and prev_l == \"PHONE_NUM\" and prev_l == next_l:\n",
    "            l = \"PHONE_NUM\"\n",
    "            \n",
    "        # Found individual mislabeled STREET_ADDRESS tokens so make sure:\n",
    "        elif l == \"STREET_ADDRESS\" and l != next_l and l != prev_l:\n",
    "            l = \"O\"\n",
    "        \n",
    "        # Create spacy tokens and their labels\n",
    "        tok_ = tokenize_with_spacy(t)\n",
    "        spacy_tokens = tok_[\"tokens\"]\n",
    "        new_tokens.extend(spacy_tokens)\n",
    "        \n",
    "        new_labels.extend([l if st not in string.punctuation else \"O\" for st in spacy_tokens])\n",
    "        \n",
    "        new_trailing_whitespaces.extend(tok_[\"trailing_whitespace\"])\n",
    "        new_trailing_whitespaces[-1] = ws  \n",
    "        \n",
    "    return pd.Series({\"document\": row.document, \"tokens\": new_tokens, \"trailing_whitespace\": new_trailing_whitespaces, \"labels\": new_labels})\n",
    "\n",
    "\n",
    "def update_labels(row):\n",
    "    '''\n",
    "    Update the labels of puntuation\n",
    "    '''\n",
    "    \n",
    "    tokens = row.tokens\n",
    "    labels = row.labels\n",
    "    new_labels = [\"O\"] * len(labels)\n",
    "    for i, (t, l) in enumerate(zip(tokens, labels)):\n",
    "        prev_l = new_labels[i - 1] if i > 0 else \"O\"\n",
    "        next_l = labels[i + 1] if i + 1 < len(labels) else \"O\"\n",
    "    \n",
    "        if (prev_l == \"NAME_STUDENT\" or prev_l == \"O\") and t == \"'s\":\n",
    "            new_labels[i] = \"O\"\n",
    "\n",
    "        elif t == \"(\" and next_l == \"PHONE_NUM\":\n",
    "            new_labels[i] = next_l\n",
    "\n",
    "        elif t == \")\" and prev_l == \"PHONE_NUM\":\n",
    "            new_labels[i] = prev_l\n",
    "        \n",
    "        # The sandwitched punctuaion between two same labels is given same label except some cases:\n",
    "        elif (t in string.punctuation) and (prev_l == next_l) and (prev_l != \"O\"):\n",
    "            \n",
    "            if t == \",\" and prev_l != \"STREET_ADDRESS\":\n",
    "                new_labels[i] = \"O\"\n",
    "            elif t == \".\" and prev_l == \"NAME_STUDENT\":\n",
    "                new_labels[i] = \"O\"\n",
    "            else:\n",
    "                new_labels[i] = prev_l\n",
    "        \n",
    "        else:\n",
    "            new_labels[i] = l\n",
    "    \n",
    "    return new_labels\n",
    "\n",
    "def create_bio_labels(labels):\n",
    "    new_labels = [\"O\"]*len(labels)\n",
    "    prev_l = \"O\"\n",
    "    for i, l in enumerate(labels):\n",
    "        if l != \"O\":\n",
    "            if l != prev_l:\n",
    "                new_labels[i] = \"B-\" + l\n",
    "            elif l == prev_l:\n",
    "                new_labels[i] = \"I-\" + l\n",
    "        prev_l = l\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pii_dataset = pii_dataset.apply(create_new_tokens_labels, axis=1)\n",
    "new_pii_dataset[\"labels\"] = new_pii_dataset.apply(update_labels, axis=1).apply(create_bio_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pii_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pii_dataset['text'] = new_pii_dataset['tokens'].apply(lambda x: ' '.join(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the two datasets\n",
    "df = pd.concat([data_df, pii_dataset[['document', 'text', 'tokens','trailing_whitespace', 'labels', 'prompt_id', 'llm']]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target distribution plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df['labels'].tolist()\n",
    "\n",
    "# Flatten the list of lists\n",
    "flattened_labels = list(itertools.chain.from_iterable(labels))\n",
    "\n",
    "# Count the occurrences of each label\n",
    "label_counts = Counter(flattened_labels)\n",
    "\n",
    "# Separate the labels and their counts for plotting\n",
    "labels, counts = zip(*label_counts.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the bar plot\n",
    "fig = go.Figure([go.Bar(x=labels[1:], y=counts[1:])])\n",
    "\n",
    "# Customize the layout\n",
    "fig.update_layout(title_text='Frequency of Each Label', xaxis_title='Labels', yaxis_title='Frequency')\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unique targets in each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['unique_labels'] = df['labels'].apply(lambda x: list(set(x)))\n",
    "df['num_labels'] = df['unique_labels'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram of number of labels per document\n",
    "fig = px.histogram(df, x='num_labels', nbins=20, title='Histogram of number of unique labels per document')\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of tokens in each document (with whitespace) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_tokens'] = df['tokens'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram of number of tokens per document\n",
    "fig = px.histogram(df, x='num_tokens', nbins=500, title='Histogram of number of tokens per document')\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of text length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['len_text'] = df['text'].apply(len)\n",
    "fig = px.histogram(df, x='len_text', nbins=500, title='Histogram of length of text per document')\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of documents without labels(only \"o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_outer = df[df['labels'].apply(lambda x: len(set(x)) > 1)] #with labels\n",
    "df_outer = df[df['labels'].apply(lambda x: 'O' in x and len(set(x)) == 1)] #without labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_outer['Label Type'] = 'With Labels'\n",
    "df_outer['Label Type'] = 'Without Labels'\n",
    "\n",
    "# Calculate text length\n",
    "df['len_text'] = df['text'].apply(len)\n",
    "df_non_outer['len_text'] = df_non_outer['text'].apply(len)\n",
    "df_outer['len_text'] = df_outer['text'].apply(len)\n",
    "\n",
    "# Combine the dataframes\n",
    "combined_df = pd.concat([df_non_outer, df_outer])\n",
    "\n",
    "# Plotting\n",
    "fig = px.histogram(combined_df, x='len_text', color='Label Type', barmode='overlay',\n",
    "                   nbins=500, title='Histogram of Length of Text per Document')\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the dataframes\n",
    "combined_df = pd.concat([df_non_outer, df_outer])\n",
    "combined_df['len_tokens'] = combined_df['tokens'].apply(len)\n",
    "# Plotting\n",
    "fig = px.histogram(combined_df, x='len_tokens', color='Label Type', barmode='overlay',\n",
    "                   nbins=500, title='Histogram of Length of Tokens per Document')\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels Pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"labels_pos\"] = df[\"labels\"].apply(lambda labels: np.arange(1, len(labels) + 1) / len(labels))\n",
    "exp_df = df.explode([\"tokens\", \"labels\", \"labels_pos\"])\n",
    "exp_df[\"labels\"] = pd.Categorical(exp_df[\"labels\"], categories=labels, ordered=True)\n",
    "exp_df = exp_df.sort_values(by=\"labels\", ascending=False)\n",
    "label_tokens = exp_df.groupby(\"labels\", observed=False).agg(list)\n",
    "label_tokens[\"counts\"] = label_tokens[\"tokens\"].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(exp_df, x='labels_pos', y='labels', title='Scatter Plot of Labels in Documents',)\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title='X Axis Label',\n",
    "    yaxis_title='Y Axis Label',\n",
    "    legend_title='Legend'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word cloud for each target \n",
    "wordcloud for surronding words of each target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(df_train['full_text']))\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud of Essays')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bio_to_spacy_entities(doc, labels):\n",
    "    \"\"\"Convert BIO-tagged labels into spaCy entity spans.\n",
    "\n",
    "    Args:\n",
    "        doc (Doc): A spaCy Doc object containing the tokenized text.\n",
    "        labels (List[str]): A list of BIO labels corresponding to each token in the Doc.\n",
    "\n",
    "    Returns:\n",
    "        List[Span]: A list of spaCy Span objects representing the entities.\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    start = None  # Track start index of the entity\n",
    "    for i, (token, label) in enumerate(zip(doc, labels)):\n",
    "        if label.startswith('B-'):\n",
    "            start = i\n",
    "        elif label.startswith('I-') and start is not None:\n",
    "            continue\n",
    "        elif start is not None:\n",
    "            # End of the entity\n",
    "            entities.append(Span(doc, start, i, label=labels[start][2:]))\n",
    "            start = None\n",
    "        if label.startswith('B-') and i == len(doc) - 1:\n",
    "            # Handle case where an entity is at the end of the text\n",
    "            entities.append(Span(doc, start, i+1, label=label[2:]))\n",
    "    if start is not None:\n",
    "        # Ensure any final entity is added\n",
    "        entities.append(Span(doc, start, len(doc), label=labels[start][2:]))\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_example(document_series, options=None, custom_css=None):\n",
    "    \"\"\"Visualize entities in a text using spaCy and displaCy.\n",
    "\n",
    "    Args:\n",
    "        document_series (dict | pd.Series): The example to visualize\n",
    "        options (dict, optional): The options for displacy rendering colours\n",
    "        custom_css (dict, optional): Additional customization\n",
    "    \"\"\"\n",
    "\n",
    "    doc = spacy_nlp(document_series['text'])  # Tokenize the text with spaCy\n",
    "    entities = bio_to_spacy_entities(doc, document_series['labels'])\n",
    "    doc.ents = entities  # Update the doc's entities\n",
    "    \n",
    "    # Define custom CSS\n",
    "    custom_css = custom_css or \"\"\"\n",
    "    <style>    \n",
    "        /* Customizing entity appearance */\n",
    "        .entities {\n",
    "            font-size: 11px !important;\n",
    "            font-family: Verdana !important;\n",
    "            line-height: 1.25 !important;\n",
    "            border-radius: 10px !important; /* Rounded corners */\n",
    "            background-color: #f9f9f9 !important; /* Very light gray background */\n",
    "            padding: 20px 15px !important; /* Adjust padding */\n",
    "        }\n",
    "        /* Customizing entity appearance */\n",
    "        .entity {\n",
    "            font-size: 10px !important;\n",
    "            padding: 0.2em 0.4em !important;\n",
    "            font-family: Verdana !important;\n",
    "            font-weight: bold !important;\n",
    "            \n",
    "        }\n",
    "    </style>\n",
    "    \"\"\"\n",
    "\n",
    "    # Inject custom CSS\n",
    "    display(HTML(custom_css))\n",
    "    \n",
    "    # Visualization\n",
    "    displacy.render(\n",
    "        doc, \n",
    "        style=\"ent\", \n",
    "        # options=options or COMPETITION_DISPLACY_OPTIONS, \n",
    "        jupyter=True\n",
    "    )\n",
    "\n",
    "visualize_example(df.iloc[-1])\n",
    "\n",
    "# print(\"\\n\\n... VISUALIZE THE EXAMPLE WITH THE MOST UNIQUE PII ...\\n\")\n",
    "# visualize_example(data_df.sort_values(by=[\"unique_pii_count\", \"total_pii_count\"], ascending=False).reset_index(drop=True).iloc[0])\n",
    "\n",
    "# print(\"\\n\\n... VISUALIZE THE EXAMPLE WITH THE MOST PII TOTAL ...\\n\")\n",
    "# visualize_example(data_df.sort_values(by=[\"total_pii_count\", \"unique_pii_count\"], ascending=False).reset_index(drop=True).iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pii_data = {\n",
    "    'EMAIL':{},\n",
    "    'ID_NUM':{},\n",
    "    'NAME_STUDENT':{},\n",
    "    'PHONE_NUM':{},\n",
    "    'STREET_ADDRESS':{},\n",
    "    'URL_PERSONAL':{},\n",
    "    'USERNAME':{},\n",
    "}\n",
    "\n",
    "def update_pii_data(pii_type, token, tokens, i):\n",
    "    if pii_type not in pii_data:\n",
    "        pii_data[pii_type] = {}\n",
    "    \n",
    "    # Extracting the token and surrounding context\n",
    "    token_text = tokens[i]\n",
    "    surrounding_tokens = tokens[max(0, i-2):i] + tokens[i+1:min(len(tokens), i+3)]\n",
    "    sentence = ' '.join(tokens)  # Simplified; consider a more accurate sentence detection\n",
    "    \n",
    "    # Assuming `tokens` is a list of all tokens in the document and `i` is the index of the current token\n",
    "    sentence_boundaries = [j for j, token in enumerate(tokens) if token in '.!?'] + [len(tokens)-1]\n",
    "    sentence_start = max([boundary for boundary in sentence_boundaries if boundary < i]+[0])\n",
    "    sentence_end = min([boundary for boundary in sentence_boundaries if boundary > i]+[len(tokens)-1])\n",
    "    sentence_context = tokens[sentence_start:sentence_end+1]\n",
    "    \n",
    "    # Determine PII position in the sentence\n",
    "    position_in_sentence = \"middle\"\n",
    "    if i == sentence_start:\n",
    "        position_in_sentence = \"beginning\"\n",
    "    elif i == sentence_end:\n",
    "        position_in_sentence = \"end\"\n",
    "    \n",
    "    # PII Token Type\n",
    "    if token_text.isalpha():\n",
    "        token_type = \"alphabetic\"\n",
    "    elif token_text.isdigit():\n",
    "        token_type = \"numeric\"\n",
    "    else:\n",
    "        token_type = \"alphanumeric\" if any(char.isalpha() for char in token_text) else \"other\"\n",
    "    \n",
    "    # PII Format Pattern\n",
    "    format_pattern = ''.join(['d' if char.isdigit() else 'l' if char.isalpha() else char for char in token_text])\n",
    "    \n",
    "    # Capitalization\n",
    "    capitalization = \"lowercase\"\n",
    "    if token_text.isupper():\n",
    "        capitalization = \"uppercase\"\n",
    "    elif token_text.istitle():\n",
    "        capitalization = \"titlecase\"\n",
    "    \n",
    "    # Special Characters\n",
    "    special_chars = any(not char.isalnum() for char in token_text)\n",
    "    \n",
    "    details = {\n",
    "        'token_text': token_text,\n",
    "        'surrounding_words': surrounding_tokens,\n",
    "        'location_in_essay': i,\n",
    "        'sentence_context': ' '.join(sentence_context),\n",
    "        'pii_length': len(token_text),\n",
    "        'position_in_sentence': position_in_sentence,\n",
    "        'token_type': token_type,\n",
    "        'format_pattern': format_pattern,\n",
    "        'capitalization': capitalization,\n",
    "        'special_chars': special_chars,\n",
    "        # Add more details as needed\n",
    "    }\n",
    "    \n",
    "    pii_token_key = f\"{token_text}_{i}\"  # Unique key for each PII instance\n",
    "    if pii_token_key not in pii_data[pii_type]:\n",
    "        pii_data[pii_type][pii_token_key] = []\n",
    "    pii_data[pii_type][pii_token_key].append(details)\n",
    "\n",
    "# Example of how to call this function within your iteration over the DataFrame\n",
    "for index, row in tqdm(train_df.iterrows(), total=len(train_df)):\n",
    "    tokens = row['tokens']\n",
    "    labels = row['labels']\n",
    "    for i, label in enumerate(labels):\n",
    "        if label != 'O':  # If the label indicates PII\n",
    "            pii_type = label[2:]  # Extract PII type (removing the B- or I- prefix)\n",
    "            update_pii_data(pii_type, tokens[i], tokens, i)\n",
    "            \n",
    "            \n",
    "# Initialize an empty list to hold each PII instance as a dictionary\n",
    "flat_pii_data = []\n",
    "\n",
    "# Iterate through the pii_data dictionary\n",
    "for pii_type, pii_instances in pii_data.items():\n",
    "    for instance_key, details_list in pii_instances.items():\n",
    "        for details in details_list:\n",
    "            # Create a flat dictionary for each PII instance\n",
    "            flat_instance = details.copy()  # Start with the existing details\n",
    "            flat_instance['pii_type'] = pii_type  # Add the PII type\n",
    "            flat_instance['instance_key'] = instance_key  # Add the instance key for reference\n",
    "            \n",
    "            # Append this flat dictionary to our list\n",
    "            flat_pii_data.append(flat_instance)\n",
    "\n",
    "# Store and viz\n",
    "train_pii_df = pd.DataFrame(flat_pii_data)\n",
    "print(\"\\n... PII DATAFRAME ...\\n\")\n",
    "display(train_pii_df)\n",
    "\n",
    "# Count the number of instances for each PII type\n",
    "pii_type_counts = train_pii_df['pii_type'].value_counts()\n",
    "print(\"\\n... PII TYPE COUNTS ...\\n\")\n",
    "display(pii_type_counts.to_frame().T)\n",
    "\n",
    "# Or aggregate to find the average PII length by type\n",
    "average_pii_length_by_type = train_pii_df.groupby('pii_type')['pii_length'].mean()\n",
    "print(\"\\n\\n... AVERAGE PII LENGTH (CHARS) BY TYPE ...\\n\")\n",
    "display(average_pii_length_by_type.to_frame().T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Detect_PII",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
