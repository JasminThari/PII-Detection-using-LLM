{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Securing Sensitive Data: Transformer model for Detecting Personally Identifiable Information\n",
    "<center>\n",
    "Advanced Business Analytics (42578) Exam Project\n",
    "<center>\n",
    "<center>\n",
    "Group: Mind Machines \n",
    "<center>\n",
    "Students: Christoffer Wejendorp (s204090) - Jasmin Thari (s204155) - Marah Marak (s182946)\n",
    "<center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure of this notebook\n",
    "\n",
    "This notebook is organized into seven distinct sections, each aimed at guiding you through various stages of the project from initial setup to in-depth analysis and discussions:\n",
    "\n",
    "1. **[Introduction](#1)**: Provides an overview of the objectives and scope of the project.\n",
    "\n",
    "2. **[Get Started](#2)**: Outlines the setup procedures including the installation of required packages within the notebook.\n",
    "\n",
    "3. **[Data & Pre-processing](#3)**: Details the dataset used in this project, followed by comprehensive steps involved in the data cleaning process to prepare the data for analysis.\n",
    "\n",
    "4. **[Exploratory Data Analysis](#4)**: Dives into the dataset through various visualization techniques to uncover patterns, trends, and insights which inform further analyses.\n",
    "\n",
    "5. **[Named Entity Recognition using Regex](#5)**: Introduces a baseline model for Named Entity Recognition (NER) utilizing regular expressions (Regex). This section demonstrates how to apply Regex patterns and rules to identify named entities within the text.\n",
    "\n",
    "6. **[Named Entity Recognition Using Transformer Model](#6)**: Advances the NER approach by implementing the DistilBert Transformer model to achieve a more sophisticated and effective entity recognition.\n",
    "\n",
    "7. **[Discussion](#7)**: Concludes with a critical analysis of the results obtained, discussing both the strengths and limitations of the methods used and suggesting potential areas for future work.\n",
    "\n",
    "### Table of Contents\n",
    "1. **[Introduction](#1)**\n",
    "2. **[Get Started](#2)**\n",
    "3. **[Exploratory Data Analysis](#4)**\n",
    "4. **[Exploratory Data Analysis](#4)**\n",
    "5. **[Named Entity Recognition using Regex](#5)**\n",
    "6. **[Named Entity Recognition Using Transformer Model](#6)**\n",
    "7. **[Discussion](#7)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## Section 1: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the age of rapid advancements in artificial intelligence and generative AI technologies, the ability to responsibly manage sensitive information is more crucial than ever. This project, *Securing Sensitive Data: AI Methods for Detecting Personally Identifiable Information*, aims to address the growing concern surrounding the protection of personal data, especially in contexts where large language models (LLMs) are employed. A primary example is the use of student essays by universities wishing to train AI models. These essays often contain personally identifiable information (PII) that must be carefully identified and redacted to maintain confidentiality and comply with data protection laws.\n",
    "\n",
    "The motivation behind this project is driven by the need to balance the utilization of generative AI in educational settings with stringent data security measures. As universities and other institutions increasingly rely on AI to enhance learning and research, ensuring the privacy of individuals represented in training datasets becomes essential. To handle this challenge, we will deploy Named Entity Recognition (NER) techniques to effectively identify and remove PII from texts.\n",
    "\n",
    "Our approach begins with the application of regular expressions (Regex), a basic yet powerful tool for pattern recognition in texts. This will serve as our baseline model for detecting straightforward instances of PII. Subsequently, we will enhance the model by incorporating a more sophisticated method using the DistilBert Transformer model. Through these methods, we aim to create a safer data environment that respects individual privacy while enabling the progressive use of AI in education.\n",
    "\n",
    "The methodologies and models developed here are not only applicable to academic settings but can also be extended to corporate environments where data privacy is paramount. As companies increasingly integrate AI technologies such as chatbots to interact with customers, the need to comply with strict regulations like the General Data Protection Regulation (GDPR) becomes critical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## Section 2: Get Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import json\n",
    "import itertools\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import math\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import ast\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from IPython.display import HTML\n",
    "\n",
    "# NLP\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.tokens import Doc, Span\n",
    "from spacy.lang.en import English\n",
    "spacy_nlp = spacy.load('en_core_web_sm')\n",
    "eng_tokenizer = English().tokenizer\n",
    "\n",
    "# Text Processing and Analysis\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "import random\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Import custom functions\n",
    "from Functions.Spacy_Tokenizer import adjust_token_labels, refine_punctuation_labels, create_bio_labels\n",
    "from Functions.Ner_Visualizer import *\n",
    "from Functions.tdidf_wordclouds import *\n",
    "\n",
    "# Ignore warnings\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Define the custom color theme\n",
    "color_theme = {\n",
    "    'three_colors': ['#57634B', '#D4793A', '#527184'],  \n",
    "    'four_colors': ['#85977D', '#8498A5', '#587F86', '#BD8A3D'],  \n",
    "    'five_colors': ['#57634B', '#D4793A', '#527184', '#CFA802', '#BBB599'], \n",
    "    'twelve_colors': ['#57634B', '#85977D', '#8498A5', '#527184', '#E9B649', '#BD8A3D', '#D4793A', \n",
    "                      '#7D1F1D', '#BB6D71', '#BBB599', '#BE477D', '#CDADE6']}\n",
    "\n",
    "# fig = go.Figure()\n",
    "# # Adding bars with specified colors from a palette\n",
    "# for i, value in enumerate([2, 3, 1, 4]):\n",
    "#     fig.add_trace(go.Bar(x=[f'Category {i+1}'], y=[value], marker_color=color_theme['four_colors'][i]))\n",
    "# fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## Section 1: Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project uses data from a Kaggle competition titled *The Learning Agency Lab - PII Data Detection*. The primary dataset includes approximately 6,807 essays contributed by students from an online course, each essay responding to a single assignment that tasked students with applying course material to a real-world problem. The objective is to annotate personally identifiable information (PII) within each essay. To preserve privacy, all reak PII has been replaced with surrogate identifiers using a semi-automated process. You can access this dataset [here](https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data).\n",
    "\n",
    "Given that the initial dataset predominantly included the *B-NAME_STUDENT* class and lacks diversity in PII types, additional data was necessary. This supplementary dataset is generated by a Large Language Model (LLM) and us also available on Kaggle. The dataset consists of 4,434 texts complete with annotations similar to the original training data. These texts, generated across eight different prompts, vary from life summaries to narratives. You can find this dataset [here](https://www.kaggle.com/datasets/alejopaullier/pii-external-dataset). Both datasets will be utilized for this project.\n",
    "\n",
    "### Project Goals\n",
    "The aim of this project is to identify and annotate various types of PII, which include:\n",
    "\n",
    "- **Student Names:** Identifying names specific to students, excluding names of instructors, authors, or other individuals.\n",
    "- **Student Emails:** Detecting email addresses belonging to students.\n",
    "- **Student Username:** Recognizing usernames associated with students.\n",
    "- **Student ID Number:** Identifying students' ID numbers or social security numbers.\n",
    "- **Student Phone Number:** Detecting phone numbers linked to students.\n",
    "- **Personal URL:** Recognizing URLs that could potentially identify students.\n",
    "- **Student Address:** Identifying street addresses related to students.\n",
    "\n",
    "### Data Structure\n",
    "The data provided includes detailed information about each essay:\n",
    "\n",
    "- **index (int):** An index number assigned to each essay.\n",
    "- **document id (int):** A unique integer identifier for each essay.\n",
    "- **full_text (string):** The complete text of each essay in UTF-8 format.\n",
    "- **tokens (list):** A sequence of tokens, derived using the SpaCy English tokenizer.\n",
    "- **trailing_whitespace (list):** A list indicating whether a space follows each token.\n",
    "- **labels (list):** These labels classify each token according to the type of PII they represent, using the BIO (Beginning, Inner, Outer) format:\n",
    "  - **B-** prefix denotes the start of a PII entity.\n",
    "  - **I-** indicates continuation of a PII entity.\n",
    "  - **O** represents tokens unrelated to PII."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Loading the first data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 6807\n"
     ]
    }
   ],
   "source": [
    "# Load data in dictionary and dataframe format\n",
    "with open('data/train.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "data_df = pd.read_json('data/train.json')\n",
    "\n",
    "print('Number of documents:',len(data_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>full_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>trailing_whitespace</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>Design Thinking for innovation reflexion-Avril...</td>\n",
       "      <td>[Design, Thinking, for, innovation, reflexion,...</td>\n",
       "      <td>[True, True, True, True, False, False, True, F...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B-NAME_STUDENT, I-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>Diego Estrada\\n\\nDesign Thinking Assignment\\n\\...</td>\n",
       "      <td>[Diego, Estrada, \\n\\n, Design, Thinking, Assig...</td>\n",
       "      <td>[True, False, False, True, True, False, False,...</td>\n",
       "      <td>[B-NAME_STUDENT, I-NAME_STUDENT, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   document                                          full_text  \\\n",
       "0         7  Design Thinking for innovation reflexion-Avril...   \n",
       "1        10  Diego Estrada\\n\\nDesign Thinking Assignment\\n\\...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [Design, Thinking, for, innovation, reflexion,...   \n",
       "1  [Diego, Estrada, \\n\\n, Design, Thinking, Assig...   \n",
       "\n",
       "                                 trailing_whitespace  \\\n",
       "0  [True, True, True, True, False, False, True, F...   \n",
       "1  [True, False, False, True, True, False, False,...   \n",
       "\n",
       "                                              labels  \n",
       "0  [O, O, O, O, O, O, O, O, O, B-NAME_STUDENT, I-...  \n",
       "1  [B-NAME_STUDENT, I-NAME_STUDENT, O, O, O, O, O...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Load LLM generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 4434\n"
     ]
    }
   ],
   "source": [
    "# Load LLM generated data in dataframe format\n",
    "llm_data_df = pd.read_csv('data/pii_dataset.csv')\n",
    "print('Number of documents:',len(llm_data_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>trailing_whitespace</th>\n",
       "      <th>labels</th>\n",
       "      <th>prompt</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>name</th>\n",
       "      <th>email</th>\n",
       "      <th>phone</th>\n",
       "      <th>job</th>\n",
       "      <th>address</th>\n",
       "      <th>username</th>\n",
       "      <th>url</th>\n",
       "      <th>hobby</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1073d46f-2241-459b-ab01-851be8d26436</td>\n",
       "      <td>My name is Aaliyah Popova, and I am a jeweler ...</td>\n",
       "      <td>['My', 'name', 'is', 'Aaliyah', 'Popova,', 'an...</td>\n",
       "      <td>[True, True, True, True, True, True, True, Tru...</td>\n",
       "      <td>['O', 'O', 'O', 'B-NAME_STUDENT', 'I-NAME_STUD...</td>\n",
       "      <td>\\n    Aaliyah Popova is a jeweler with 13 year...</td>\n",
       "      <td>1</td>\n",
       "      <td>Aaliyah Popova</td>\n",
       "      <td>aaliyah.popova4783@aol.edu</td>\n",
       "      <td>(95) 94215-7906</td>\n",
       "      <td>jeweler</td>\n",
       "      <td>97 Lincoln Street</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Podcasting</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5ec717a9-17ee-48cd-9d76-30ae256c9354</td>\n",
       "      <td>My name is Konstantin Becker, and I'm a develo...</td>\n",
       "      <td>['My', 'name', 'is', 'Konstantin', 'Becker,', ...</td>\n",
       "      <td>[True, True, True, True, True, True, True, Tru...</td>\n",
       "      <td>['O', 'O', 'O', 'B-NAME_STUDENT', 'I-NAME_STUD...</td>\n",
       "      <td>\\n    Konstantin Becker is a developer with 2 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Konstantin Becker</td>\n",
       "      <td>konstantin.becker@gmail.com</td>\n",
       "      <td>0475 4429797</td>\n",
       "      <td>developer</td>\n",
       "      <td>826 Webster Street</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Quilting</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               document  \\\n",
       "0  1073d46f-2241-459b-ab01-851be8d26436   \n",
       "1  5ec717a9-17ee-48cd-9d76-30ae256c9354   \n",
       "\n",
       "                                                text  \\\n",
       "0  My name is Aaliyah Popova, and I am a jeweler ...   \n",
       "1  My name is Konstantin Becker, and I'm a develo...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  ['My', 'name', 'is', 'Aaliyah', 'Popova,', 'an...   \n",
       "1  ['My', 'name', 'is', 'Konstantin', 'Becker,', ...   \n",
       "\n",
       "                                 trailing_whitespace  \\\n",
       "0  [True, True, True, True, True, True, True, Tru...   \n",
       "1  [True, True, True, True, True, True, True, Tru...   \n",
       "\n",
       "                                              labels  \\\n",
       "0  ['O', 'O', 'O', 'B-NAME_STUDENT', 'I-NAME_STUD...   \n",
       "1  ['O', 'O', 'O', 'B-NAME_STUDENT', 'I-NAME_STUD...   \n",
       "\n",
       "                                              prompt  prompt_id  \\\n",
       "0  \\n    Aaliyah Popova is a jeweler with 13 year...          1   \n",
       "1  \\n    Konstantin Becker is a developer with 2 ...          1   \n",
       "\n",
       "                name                        email            phone        job  \\\n",
       "0     Aaliyah Popova   aaliyah.popova4783@aol.edu  (95) 94215-7906    jeweler   \n",
       "1  Konstantin Becker  konstantin.becker@gmail.com     0475 4429797  developer   \n",
       "\n",
       "              address username  url       hobby  len  \n",
       "0   97 Lincoln Street      NaN  NaN  Podcasting  363  \n",
       "1  826 Webster Street      NaN  NaN    Quilting  255  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_data_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> - Let's explore the number of different prompt IDs present in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(llm_data_df['prompt_id'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Below, we display a random sample for each prompt ID, where the prompt ID represents a category, each prompt within the same category is unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>Prompt ID</th><th>Text</th></tr><tr><td>0</td><td>\n",
       "    Write a fictional semi-formal biography in first person for Tao Kato. Add the following information about him/her randomly inside the text: name is Tao Kato, phone number is (960) 464-3988, email is tao.kato8489@gmail.edu, address is 1666 South Summer Rose Avenue.\n",
       "    </td></tr><tr><td>1</td><td>\n",
       "    Chen Mitsubishi is a musician with 12 years of experience. Write a detailed example in first person of a job-related project he/her did in the past. Add the following information about him/her randomly inside the text: name is Chen Mitsubishi, phone number is 0438 437 5019, email is chenmitsubishi@gmail.org, hobby is Painting, address is 11813 West 75th Circle.\n",
       "    </td></tr><tr><td>2</td><td>\n",
       "    Viktor Suzuki is a neurologist. Write about a job-related project he/her did in the past including some of the following information: phone number is +91-25517 28305, email is viktorsuzuki5689@outlook.net\n",
       "    </td></tr><tr><td>3</td><td>\n",
       "    Mohammed Martinez is a dietician. Write a first person summary of something he solved in his job. Add the following information about him/her (randomly please) inside the text: name is Mohammed Martinez, email is mohammed_martinez@yahoo.edu, address is 235 Hugh Thomas Drive.\n",
       "    </td></tr><tr><td>4</td><td>\n",
       "    Write a fictional semi-formal biography in first person for Omar Mitsubishi. Add the following information about him/her randomly inside the text: name is Omar Mitsubishi, profile at X.com is omar_mitsubishi19, email is omarmitsubishi@gmail.com, webpage is https://www.omarmitsubishi.edu/news. It is important to include this information in different parts of the text.\n",
       "    </td></tr><tr><td>5</td><td>\n",
       "    Anil Kobayashi is a coach with 3 years of experience. Write a detailed example in first person of a job-related project he/her did in the past. Add the following information about him/her randomly inside the text: name is Anil Kobayashi, webpage is www.anil_kobayashi.com/news.php, profile at LinkedIn is anilkobayashi, address is 4674 Amy Landing Suite 292.\n",
       "    </td></tr><tr><td>6</td><td>\n",
       "    Angel Roux is a biologist. Write about a job-related project he/her did in the past including some of the following information: phone number is 080-8030-5392, profile at LinkedIn is a.roux. It is important to include this information randomly throughout the text.\n",
       "    </td></tr><tr><td>7</td><td>\n",
       "    Ram Rousseau is a psychologist. Write a first person summary of something he solved in his job. Add the following information about him/her (randomly please) inside the text: name is Ram Rousseau, email is ram_rousseau6798@yahoo.net, address is 824 Peters Neck Apt. 294, profile at Instagram is ram.rousseau98, webpage is www.ram-rousseau.biz.\n",
       "    </td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Group by 'prompt_id', sample one record from each group, and reset index to flatten the DataFrame\n",
    "sampled_prompts = llm_data_df.groupby('prompt_id').apply(lambda x: x.sample(1), include_groups=False).reset_index(drop=False)\n",
    "html = '<table>'\n",
    "html += '<tr><th>Prompt ID</th><th>Text</th></tr>'  \n",
    "for index, row in sampled_prompts.iterrows():\n",
    "    html += f'<tr><td>{row[\"prompt_id\"]}</td><td>{row[\"prompt\"]}</td></tr>'\n",
    "\n",
    "html += '</table>'\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - From the output, we can observe that the majority of the prompts are either related to historical writing or summaries of job-related topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - First, we will preprocess both datasets to ensure they are aligned. For example, we will rename the text columns in both datasets to \"text\", add a new column to track which data is generated by LLMs, and assign a prompt ID of -1 to the non-generated data to facilitate tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df.rename(columns={'full_text':'text'}) # Rename column to 'text'\n",
    "data_df['llm_generated'] = False # Add column to indicate if the text was generated by LLM\n",
    "data_df['prompt_id'] = -1  # Add column to indicate the prompt id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_data_df['llm_generated'] = True # Add column to indicate if the text was generated by LLM\n",
    "\n",
    "llm_data_df[[\"tokens\", \"trailing_whitespace\", \"labels\"]] = llm_data_df[[\"tokens\", \"trailing_whitespace\", \"labels\"]].map(ast.literal_eval) # Convert string to list\n",
    "llm_data_df[\"document\"] = llm_data_df[\"document\"].astype(\"category\").cat.codes + (data_df.document.max() + 1) # make sure document id is unique and changing to int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Addressing the Punctuation Issue in the LLM generated data set**:\n",
    "\n",
    "> In the context of text processing, particularly when handling datasets for named entity recognition (NER), punctuation plays a crucial role in determining the boundaries and labels of tokens. The standard tokenization by tools like SpaCy segment a phrase such as \"Charles, by\" into [\"Charles\", \",\", \"by\"] with trailing spaces marked as [False, True, True], and labels [\"B-NAME-STUDENT\", \"O\", \"O\"]. This segmentation accurately reflects the presence of punctuation as separate from named entities, even when there's no space between them.\n",
    ">\n",
    "> However, the LLM generated dataset presents a unique challenge, where all token are seperated by space so its tokenization will be [\"Charles,\", \"by\"] [True, True] [\"B-NAME-STUDENT\", \"O\"] and thus tokens without space are considered single token and are given single labels. While the tokenization will leas to the same text e.g [\"Char\", \"les\", \",\", \"_by\"], the model labels will be different [\"B-NAME-STUDENT\", \"B-NAME-STUDENT\", \"B-NAME-STUDENT\", \"O\"] instead of appropriate [\"B-NAME-STUDENT\", \"B-NAME-STUDENT\", \"O\", \"O\"]. \n",
    ">\n",
    ">So, this approach results in punctuation being considered part of a preceding token, thus receiving a single, unified label. Consequently, when aligning the LLM dataset's tokenization with the original, we encounter difference.\n",
    ">\n",
    ">To resolve this, adjustments are necessary to ensure that the dataset aligns more closely with standard tokenization and labeling practices. This involves re-evaluating tokens and labels to correctly identify and separate punctuation from named entities, thereby avoiding the mislabeling that can confuse NER models. By addressing this issue, we aim to improve the dataset's utility for training more accurate and reliable NER systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NB! The following code uses functions from `Spacy_Tokenizer.py` file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_data_df_tokenized = llm_data_df.apply(adjust_token_labels, axis=1)\n",
    "llm_data_df_tokenized[\"labels\"] = llm_data_df_tokenized.apply(refine_punctuation_labels, axis=1).apply(create_bio_labels)\n",
    "llm_data_df_tokenized['text'] = llm_data_df['tokens'].apply(lambda x: ' '.join(x))\n",
    "llm_data_df_tokenized[['prompt_id', 'llm_generated']] = llm_data_df[['prompt_id', 'llm_generated']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>tokens</th>\n",
       "      <th>trailing_whitespace</th>\n",
       "      <th>labels</th>\n",
       "      <th>text</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>llm_generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22968</td>\n",
       "      <td>[My, name, is, Aaliyah, Popova, ,, and, I, am,...</td>\n",
       "      <td>[True, True, True, True, False, True, True, Tr...</td>\n",
       "      <td>[O, O, O, B-NAME_STUDENT, I-NAME_STUDENT, O, O...</td>\n",
       "      <td>My name is Aaliyah Popova, and I am a jeweler ...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24398</td>\n",
       "      <td>[My, name, is, Konstantin, Becker, ,, and, I, ...</td>\n",
       "      <td>[True, True, True, True, False, True, True, Fa...</td>\n",
       "      <td>[O, O, O, B-NAME_STUDENT, I-NAME_STUDENT, O, O...</td>\n",
       "      <td>My name is Konstantin Becker, and I'm a develo...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   document                                             tokens  \\\n",
       "0     22968  [My, name, is, Aaliyah, Popova, ,, and, I, am,...   \n",
       "1     24398  [My, name, is, Konstantin, Becker, ,, and, I, ...   \n",
       "\n",
       "                                 trailing_whitespace  \\\n",
       "0  [True, True, True, True, False, True, True, Tr...   \n",
       "1  [True, True, True, True, False, True, True, Fa...   \n",
       "\n",
       "                                              labels  \\\n",
       "0  [O, O, O, B-NAME_STUDENT, I-NAME_STUDENT, O, O...   \n",
       "1  [O, O, O, B-NAME_STUDENT, I-NAME_STUDENT, O, O...   \n",
       "\n",
       "                                                text  prompt_id  llm_generated  \n",
       "0  My name is Aaliyah Popova, and I am a jeweler ...          1           True  \n",
       "1  My name is Konstantin Becker, and I'm a develo...          1           True  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_data_df_tokenized.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thus, the first row's tokens are transformed from 411 to 363 tokens and are more aligned with the first data set and the more approciate tokenizer.\n"
     ]
    }
   ],
   "source": [
    "print(\"Thus, the first row's tokens are transformed from \" + str(len(llm_data_df_tokenized['tokens'].iloc[0])) + \" to \" + str(len(llm_data_df['tokens'].iloc[0])) + \" tokens and are more aligned with the first data set and the more approciate tokenizer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Combine data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Both that data sets have now been pre-processed and we are ready to combine them into one data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([data_df, llm_data_df_tokenized], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 11241\n"
     ]
    }
   ],
   "source": [
    "print('Number of documents:',len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>trailing_whitespace</th>\n",
       "      <th>labels</th>\n",
       "      <th>llm_generated</th>\n",
       "      <th>prompt_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>Design Thinking for innovation reflexion-Avril...</td>\n",
       "      <td>[Design, Thinking, for, innovation, reflexion,...</td>\n",
       "      <td>[True, True, True, True, False, False, True, F...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B-NAME_STUDENT, I-...</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>Diego Estrada\\n\\nDesign Thinking Assignment\\n\\...</td>\n",
       "      <td>[Diego, Estrada, \\n\\n, Design, Thinking, Assig...</td>\n",
       "      <td>[True, False, False, True, True, False, False,...</td>\n",
       "      <td>[B-NAME_STUDENT, I-NAME_STUDENT, O, O, O, O, O...</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   document                                               text  \\\n",
       "0         7  Design Thinking for innovation reflexion-Avril...   \n",
       "1        10  Diego Estrada\\n\\nDesign Thinking Assignment\\n\\...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [Design, Thinking, for, innovation, reflexion,...   \n",
       "1  [Diego, Estrada, \\n\\n, Design, Thinking, Assig...   \n",
       "\n",
       "                                 trailing_whitespace  \\\n",
       "0  [True, True, True, True, False, False, True, F...   \n",
       "1  [True, False, False, True, True, False, False,...   \n",
       "\n",
       "                                              labels  llm_generated  prompt_id  \n",
       "0  [O, O, O, O, O, O, O, O, O, B-NAME_STUDENT, I-...          False         -1  \n",
       "1  [B-NAME_STUDENT, I-NAME_STUDENT, O, O, O, O, O...          False         -1  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Encode target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The last step, will be to encode the targets as one hot encoding. \n",
    ">\n",
    "> !NB The function used from `Spacy_Tokenizer.py` file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(df):\n",
    "    df = df.copy()\n",
    "    df[\"unique_labels\"] = df[\"labels\"].apply(lambda x: set(\n",
    "        [l.split('-')[1] if l != 'O' else l for l in x]\n",
    "         ))\n",
    "\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    one_hot_encoded = mlb.fit_transform(df['unique_labels'])\n",
    "    one_hot_df = pd.DataFrame(one_hot_encoded, columns=mlb.classes_)\n",
    "    df = pd.concat([df, one_hot_df], axis=1)\n",
    "    \n",
    "    # add 'OTHER' column which is only true when we have no other label in text\n",
    "    df['OTHER'] = df['unique_labels'].apply(lambda x: 1 if len(x - {\"O\"}) == 0 else 0)\n",
    "    \n",
    "    return df, list(mlb.classes_) + ['OTHER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, label_classes = encode_labels(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>trailing_whitespace</th>\n",
       "      <th>labels</th>\n",
       "      <th>llm_generated</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>unique_labels</th>\n",
       "      <th>EMAIL</th>\n",
       "      <th>ID_NUM</th>\n",
       "      <th>...</th>\n",
       "      <th>USERNAME</th>\n",
       "      <th>OTHER</th>\n",
       "      <th>EMAIL</th>\n",
       "      <th>ID_NUM</th>\n",
       "      <th>NAME_STUDENT</th>\n",
       "      <th>O</th>\n",
       "      <th>PHONE_NUM</th>\n",
       "      <th>STREET_ADDRESS</th>\n",
       "      <th>URL_PERSONAL</th>\n",
       "      <th>USERNAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>Design Thinking for innovation reflexion-Avril...</td>\n",
       "      <td>[Design, Thinking, for, innovation, reflexion,...</td>\n",
       "      <td>[True, True, True, True, False, False, True, F...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B-NAME_STUDENT, I-...</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>{NAME_STUDENT, O}</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>Diego Estrada\\n\\nDesign Thinking Assignment\\n\\...</td>\n",
       "      <td>[Diego, Estrada, \\n\\n, Design, Thinking, Assig...</td>\n",
       "      <td>[True, False, False, True, True, False, False,...</td>\n",
       "      <td>[B-NAME_STUDENT, I-NAME_STUDENT, O, O, O, O, O...</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>{NAME_STUDENT, O}</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   document                                               text  \\\n",
       "0         7  Design Thinking for innovation reflexion-Avril...   \n",
       "1        10  Diego Estrada\\n\\nDesign Thinking Assignment\\n\\...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [Design, Thinking, for, innovation, reflexion,...   \n",
       "1  [Diego, Estrada, \\n\\n, Design, Thinking, Assig...   \n",
       "\n",
       "                                 trailing_whitespace  \\\n",
       "0  [True, True, True, True, False, False, True, F...   \n",
       "1  [True, False, False, True, True, False, False,...   \n",
       "\n",
       "                                              labels  llm_generated  \\\n",
       "0  [O, O, O, O, O, O, O, O, O, B-NAME_STUDENT, I-...          False   \n",
       "1  [B-NAME_STUDENT, I-NAME_STUDENT, O, O, O, O, O...          False   \n",
       "\n",
       "   prompt_id      unique_labels  EMAIL  ID_NUM  ...  USERNAME  OTHER  EMAIL  \\\n",
       "0         -1  {NAME_STUDENT, O}      0       0  ...         0      0      0   \n",
       "1         -1  {NAME_STUDENT, O}      0       0  ...         0      0      0   \n",
       "\n",
       "   ID_NUM  NAME_STUDENT  O  PHONE_NUM  STREET_ADDRESS  URL_PERSONAL  USERNAME  \n",
       "0       0             1  1          0               0             0         0  \n",
       "1       0             1  1          0               0             0         0  \n",
       "\n",
       "[2 rows x 25 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## Section 2: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Target Distrbution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we are examining the frequency of each target in all the documents excluding target \"O\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df['labels'].tolist()\n",
    "# Flatten the list of lists\n",
    "flattened_labels = list(itertools.chain.from_iterable(labels))\n",
    "# Count the occurrences of each label\n",
    "label_counts = Counter(flattened_labels)\n",
    "# Separate the labels and their counts for plotting\n",
    "labels, counts = zip(*label_counts.items())\n",
    "\n",
    "# Create the bar plot\n",
    "fig = go.Figure([go.Bar(x=labels[1:], y=counts[1:])])\n",
    "fig.update_layout(title_text='Frequency of each label', xaxis_title='Labels', yaxis_title='Frequency')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is evident that the most common targets are *B-NAME-STUDENT*, *I-NAME-STUDENT*, *B-STREET_ADDRESS*, and *I-STREET_ADDRESS*. It is not surprising that *NAME-STUDENT* is the most frequently occurring target.\n",
    "- It is also observed that some targets are very rare, such as *B-ID_NUM*, *I-URL_PERSONAL*, and *I-ID_NUM*.\n",
    "- We notice the absence of certain potential targets like *I-EMAIL* and *I-USERNAME*. However, these targets are less common since emails or usernames extending beyond a single word are atypical.\n",
    "- Overall, we have a diverse array of targets, which is crucial for effective model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we are examining the distribution of unqiue variables in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['unique_labels'] = df['labels'].apply(lambda x: list(set(x)))\n",
    "df['num_unique_labels'] = df['unique_labels'].apply(len)\n",
    "\n",
    "# Histogram of number of unique labels per document\n",
    "fig = px.histogram(df, x='num_unique_labels', nbins=20, \n",
    "                   labels={'num_unique_labels': 'Number of unique labels'},\n",
    "                   title='Histogram of number of unique labels per document')\n",
    "fig.update_layout(yaxis_title='Frequency') \n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is seen that it is most common for each document to have at least one unique target. \n",
    "- It is also noticed that for some reason some texts do also have up to 6 or 8 unique labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also interesting to see how many targets there are in each document when excluding label \"O\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_labels'] = df['labels'].apply(lambda labels: len([label for label in labels if label != \"O\"]))\n",
    "filtered_df = df[df['num_labels'] > 0]\n",
    "# Histogram of number of unique labels per document\n",
    "fig = px.histogram(filtered_df, x='num_labels', nbins=50, \n",
    "                   labels={'num_labels': 'Number of labels'},\n",
    "                   title='Histogram of number of labels per document')\n",
    "fig.update_layout(yaxis_title='Frequency') \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of documents without any target:\", len(df['num_labels']==0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is seen that we are around 11241 documents without any targets. \n",
    "- Also, we have some few documents with many targets. \n",
    "- Otherwise we see that most of the documents have around 5-15 targets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Document distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we are visualizing the length of the text of both the documents with and without labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_labels = df[df['labels'].apply(lambda x: len(set(x)) > 1)] #with labels\n",
    "df_non_labels = df[df['labels'].apply(lambda x: 'O' in x and len(set(x)) == 1)] #without labels \n",
    "\n",
    "df_with_labels['Documents'] = 'With Labels'\n",
    "df_non_labels['Documents'] = 'Without Labels'\n",
    "df['Documents'] = \"All Documents\"\n",
    "\n",
    "# Calculate text length\n",
    "df['len_text'] = df['text'].apply(len)\n",
    "df_with_labels['len_text'] = df_with_labels['text'].apply(len)\n",
    "df_non_labels['len_text'] = df_non_labels['text'].apply(len)\n",
    "\n",
    "# Combine the dataframes\n",
    "combined_df = pd.concat([df_with_labels, df_non_labels, df])\n",
    "\n",
    "# Plotting\n",
    "fig = px.histogram(combined_df, x='len_text', color='Documents', labels={'len_text': 'Length of text'},\n",
    "                   nbins=500, title='Histogram of Length of Text per Document')\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is clear that the distribution of text length is approximating a heavy tailed normal distribution for all three."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let us see the length of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['Length of tokens'] = combined_df['tokens'].apply(len)\n",
    "# Plotting\n",
    "fig = px.histogram(combined_df, x='Length of tokens', color='Documents',\n",
    "                   nbins=500, title='Histogram of Length of Tokens per Document')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is seen that all three distrbutions are quiet similar. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 POS Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we will calculates normalized positions for each label in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Labels Pos\"] = df[\"labels\"].apply(lambda labels: np.arange(1, len(labels) + 1) / len(labels))\n",
    "exp_df = df.explode([\"tokens\", \"labels\", \"Labels Pos\"])\n",
    "exp_df[\"labels\"] = pd.Categorical(exp_df[\"labels\"], categories=labels, ordered=True)\n",
    "exp_df = exp_df.sort_values(by=\"labels\", ascending=False)\n",
    "label_tokens = exp_df.groupby(\"labels\", observed=False).agg(list)\n",
    "label_tokens[\"counts\"] = label_tokens[\"tokens\"].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(exp_df, x='Labels Pos', y='labels', title='Scatter Plot of Labels in Documents',)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Named Entity Recognition (NER) using spaCy \n",
    "\n",
    "In the following, we will visualize text data with their corresponding labels using NER spaCy library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First, we are presenting a text that contains the highest number of unique labels, which, in our dataset, reached a maximum of eight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_ner(df.sort_values(by=[\"num_unique_labels\"], ascending=False).reset_index(drop=True).iloc[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=[\"num_unique_labels\"], ascending=False)['unique_labels'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The illustration demonstrates the accurate labeling of the text. Additionally, it highlights the use of labels that indicate both the beginning of the target entity and its continuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the example below, we showcase text featuring multiple labels. Notably, the sequence includes an email address after the PHONE_NUM class ,which are not labeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_ner(df[df['document'] == 9854])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The interesting aspect is determining whether the unlabeled email is a personal email or an oversight in labeling. Given the semi-automated nature of the labeling process, we anticipate encountering several such errors in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lastly, we are showing one of the texts with most labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_ner(df.sort_values(by=[\"num_labels\"], ascending=False).reset_index(drop=True).iloc[1:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 WordClouds using tf–idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First, we will investigate the wordclouds using all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = preprocess_texts(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_documents = [doc.lower().split() for doc in documents]\n",
    "tfidf_documents = calc_td_idf(tokenize_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wordcloud(\"WordCloud of all documents\", tfidf_documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Wordcloud of target Student Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_tokens_per_document = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    document_tokens = [token for token, label in zip(row['tokens'], row['labels']) if label in ['B-NAME_STUDENT', 'I-NAME_STUDENT']]\n",
    "    extracted_tokens_per_document.append(document_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_names = [[word.lower() for word in sublist] for sublist in extracted_tokens_per_document]\n",
    "tfidf_names = calc_td_idf(tokenize_names)\n",
    "tfidf_names = {k: v for d in tfidf_names for k, v in d.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wordcloud(\"WordCloud of student names\", tfidf_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tokens_before_labels(data_df, target_label, num_tokens=2):\n",
    "    all_tokens_before_label = []\n",
    "    \n",
    "    for _, row in data_df.iterrows():\n",
    "        tokens, labels = row['tokens'], row['labels']\n",
    "        tokens_before_label = [\n",
    "            tokens[i-num_tokens:i] \n",
    "            for i, label in enumerate(labels) \n",
    "            if label == target_label and i-num_tokens >= 0\n",
    "        ]\n",
    "        all_tokens_before_label.extend(tokens_before_label)\n",
    "    \n",
    "    return all_tokens_before_label\n",
    "\n",
    "# Extracting the two tokens before 'B-NAME_STUDENT'\n",
    "tokens_before_B_NAME_STUDENT = extract_tokens_before_labels(df, 'B-NAME_STUDENT')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_before_B_NAME_STUDENT = [[word.lower() for word in sublist] for sublist in tokens_before_B_NAME_STUDENT]\n",
    "tfidf_before_names = calc_td_idf(tokens_before_B_NAME_STUDENT)\n",
    "tfidf_before_names = {k: v for d in tfidf_before_names for k, v in d.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wordcloud(\"WordCloud of student names\", tfidf_before_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from itertools import chain\n",
    "\n",
    "# Assuming 'extracted_tokens_per_document' is your list of lists of tokens\n",
    "cleaned_tokens_per_document = [\n",
    "    [re.sub(r'[^\\w\\s]', '', token) for token in sublist]\n",
    "    for sublist in tokens_before_B_NAME_STUDENT\n",
    "]\n",
    "\n",
    "# Optionally, you might want to remove empty tokens that result from this cleaning\n",
    "cleaned_tokens_per_document = [\n",
    "    [token for token in sublist if token.strip()]\n",
    "    for sublist in cleaned_tokens_per_document\n",
    "]\n",
    "\n",
    "\n",
    "all_tokens = [token for sublist in cleaned_tokens_per_document for token in sublist]\n",
    "\n",
    "# Count the frequencies of each word\n",
    "word_counts = Counter(all_tokens)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the most common words and their counts\n",
    "num_most_common_words = 10  # Change this to plot more or fewer words\n",
    "most_common_words = word_counts.most_common(num_most_common_words)\n",
    "words, frequencies = zip(*most_common_words)  # This unpacks the list of tuples into two tuples\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 8))  # Adjust the figure size as needed\n",
    "plt.bar(words, frequencies, color='skyblue')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45)  # Rotate the x-axis labels to make them readable\n",
    "plt.title('Top {} Most Common Words'.format(num_most_common_words))\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Detect_PII",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
